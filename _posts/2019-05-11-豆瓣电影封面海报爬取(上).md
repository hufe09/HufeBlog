---
layout: post
title: "「爬虫」豆瓣电影封面海报爬取(上)"
subtitle: 'Python Web 爬虫'
author: "Hufe"
header-img: "img/post-bg-python.jpg"
header-mask: 0.3
mathjax: true
tags:
  - Python
  - 爬虫
---

> 参考自：[极客时间 数据分析实战45讲 10丨Python爬虫：如何自动化下载王祖贤海报?](https://time.geekbang.org/column/article/76001)

爬虫的流程整个过程包括三个阶段：
- 打开网页
- 提取数据
- 保存数据

在 Python 中，这三个阶段都有对应的工具可以使用。

在“打开网页”这一步骤中，可以使用 `Requests` 访问页面，得到服务器返回给我们的数据，这里包括 HTML 页面以及 JSON 数据。

在“提取数据”这一步骤中，主要用到了两个工具。针对 HTML 页面，可以使用 XPath 进行元素定位，提取数据；针对 JSON 数据，可以使用 JSON 进行解析。

在最后一步“保存数据”中，我们可以使用 `Pandas` 保存数据，最后导出 CSV 文件。


### Requests 访问页面

`Requests` 是 Python HTTP 的客户端库，编写爬虫的时候都会用到，编写起来也很简单。它有两种访问方式：`Get` 和 `Post`。  
这两者最直观的区别就是：Get 把参数包含在 url 中，而 Post 通过 request body 来传递参数。

假设我们想访问豆瓣，那么用 Get 访问的话，代码可以写成下面这样的：

`r = requests.get('http://www.douban.com')`
代码里的“r”就是 Get 请求后的访问结果，然后我们可以使用 r.text 或 r.content 来获取 HTML 的正文。

如果我们想要使用 Post 进行表单传递，代码就可以这样写：

`r = requests.post('http://xxx.com', data = {'key':'value'})`
这里 data 就是传递的表单参数，data 的数据类型是个字典的结构，采用 key 和 value 的方式进行存储。

### XPath 定位

XPath 是 XML 的路径语言，实际上是通过元素和属性进行导航，帮我们定位位置。它有几种常用的路径表达方式。
![](https://static001.geekbang.org/resource/image/3b/ea/3bcb311361c76bfbeb90d360b21195ea.jpg)


我来给你简单举一些例子：

- `xpath(‘node’)` 选取了 node 节点的所有子节点；

- `xpath(’/div’)` 从根节点上选取 div 节点；

- `xpath(’//div’)` 选取所有的 div 节点；

- `xpath(’./div’)` 选取当前节点下的 div 节点；

- `xpath(’…’)`回到上一个节点；

- `xpath(’//@id’)` 选取所有的 id 属性；

- `xpath(’//book[@id]’)` 选取所有拥有名为 id 的属性的 book 元素；

- `xpath(’//book[@id=“abc”]’)` 选取所有 book 元素，且这些 book 元素拥有 id= "abc"的属性；

- `xpath(’//book/title | //book/price’)` 选取 book 元素的所有 title 和 price 元素。

上面只是列举了 `XPath` 的部分应用，`XPath` 的选择功能非常强大，它可以提供超过 100 个内建函数，来做匹配。我们想要定位的节点，几乎都可以使用 XPath 来选择。

使用 XPath 定位，你会用到 Python 的一个解析库 lxml。这个库的解析效率非常高，使用起来也很简便，只需要调用 HTML 解析命令即可，然后再对 HTML 进行 XPath 函数的调用。

比如我们想要定位到 HTML 中的所有列表项目，可以采用下面这段代码。

```
from lxml import etree
html = etree.HTML(html)
result = html.xpath('//li')
```

### JSON 对象

JSON 是一种轻量级的交互方式，在 Python 中有 JSON 库，可以让我们将 Python 对象和 JSON 对象进行转换。为什么要转换呢？原因也很简单。将 JSON 对象转换成为 Python 对象，我们对数据进行解析就更方便了。

![](https://static001.geekbang.org/resource/image/9a/43/9a6d6564a64cf2b1c256265eea78c543.png)


这是一段将 JSON 格式转换成 Python 对象的代码，你可以自己运行下这个程序的结果。

```
import json
jsonData = '{"a":1,"b":2,"c":3,"d":4,"e":5}';
input = json.loads(jsonData)
print (input)
```
```
{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5}
```

## 爬虫实战
本次实战从两个角度使用 Python 爬取海报，上篇是通过 JSON 数据爬取，下篇是通过 XPath 定位爬取。

## 普通爬取

本次爬取网页为`https://www.douban.com/j/search_photo?q=%E7%8E%8B%E7%A5%96%E8%B4%A4&limit20&start=0`  
即`https://www.douban.com/j/search_photo?q=王祖贤&limit=20&start=0`

![title](https://raw.githubusercontent.com/hufe09/GitNote-Images/master/gitnote/2019/05/13/1557761380378-1557761380386.png)
url 中的乱码正是中文的 url 编码，打开后，我们看到了很清爽的 JSON 格式对象，展示的形式是这样的：
```
{"images":
       [{"src": …, "author": …, "url":…, "id": …, "title": …, "width":…, "height":…},
    …
	 {"src": …, "author": …, "url":…, "id": …, "title": …, "width":…, "height":…}],
"total": 23499,
"limit": 20,
"more": true
}
```
从这个 JSON 对象中，我们能看到，王祖贤的图片一共有 23499 张，其中一次只返回了 20 张，还有更多的数据可以请求。数据被放到了 images 对象里，它是个数组的结构，每个数组的元素是个字典的类型，分别告诉了 src、author、url、id、title、width 和 height 字段，这些字段代表的含义分别是原图片的地址、作者、发布地址、图片 ID、标题、图片宽度、图片高度等信息。

有了这个 JSON 信息，你很容易就可以把图片下载下来。当然你还需要寻找 XHR 请求的 url 规律。


```python
import json
import os

import requests
from concurrent.futures import ThreadPoolExecutor, as_completed, wait, ALL_COMPLETED

query = '王祖贤'

def download(src, id):
    folder_name = query
    if not os.path.exists(folder_name):
        os.makedirs(folder_name)
    dir = './' + query + '/' + str(id) + '.jpg'
    try:
        pic = requests.get(src, timeout=10)
        fp = open(dir, 'wb')
        fp.write(pic.content)
        fp.close()
    except requests.exceptions.ConnectionError:
        print(str(id) + '无法下载')
        
for i in range(0, 1000, 20):
    print(f'---------------正在爬取第{i // 20 + 1}页---------------')
    url = 'https://www.douban.com/j/search_photo?q=' + query + '&limit20&start=' + str(i)
    html = requests.get(url).text
    response = json.loads(html, encoding='utf-8')
    j = 1
    for image in response['images']:
        print(image['src'])
        download(image['src'], image['id'])
        print(f'第{i + j}张图片')
        j += 1
```

## 增加部分细节


```python
import json
import os
import time

import requests


def download(src, img_name, query):
    """" 下载图片 """
    # print('启动下载进程，进程号[%d].' % os.getpid())
    dir = './' + query + '/' + str(img_name) + '.jpg'
    try:
        pic = requests.get(src, timeout=10)
        fp = open(dir, 'wb')
        fp.write(pic.content)
        fp.close()
        return '开始下载:' + str(img_name), src
    except requests.exceptions.ConnectionError:
        return 'ConnectionError:' + str(img_name) + '无法下载!'
    except OSError:
        return 'OSError:' + str(img_name) + '无法下载!'

def crawling_poster(url, query):
    '''解析url, 获取元素'''
    html = requests.get(url).text
    response = json.loads(html, encoding='utf-8')
    srcs = []
    ids = []
    for image in response['images']:
        srcs.append(image['src'])
        ids.append(image['id'])
    lists = []
    for src, id, q in zip(srcs, ids, [query] * len(srcs)):
        lists.append([src, id, q])
    print('启动下载进程，进程号[%d].' % os.getpid())
    start = time.time()

    for src, id in zip(srcs, ids):
        download(src, id, query)
    end = time.time()
    print(f'本次下载耗费{end - start}秒')
    return url

def main():
    query = '王祖贤'

    # 创建文件夹
    folder_name = query
    if not os.path.exists(folder_name):
        os.makedirs(folder_name)

    urls = []
    for i in range(0, 1000, 20):
        urls.append('https://www.douban.com/j/search_photo?q=' + query + '&limit20&start=' + str(i))

    start = time.time()
    #单进程
    for url in urls:
        print(crawling_poster(url, query))

    end = time.time()
    print(f'全程耗费{end - start}秒')

if __name__ == '__main__':
    main()
```

## 多线程


```python
import time
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed, wait, ALL_COMPLETED

def multi_threads_pool(num, func, lists):
    """线程池下载"""
    threads_pool = ThreadPoolExecutor(num)

    # 如果要运行的函数只有一个参数
#     threads = [threads_pool.submit(func, (src) ) for src in srcs]

    # 如果要运行的函数有多个参数，需要借助lambda函数
    # list_var1 = ['https://img3.doubanio.com/view/celebrity/s_ratio_celebrity/public/p751.webp', 'hello']
    # list_var2 = ['https://img3.doubanio.com/view/celebrity/s_ratio_celebrity/public/p2414157745.webp', 'world']
    # threads_pool.submit(lambda p: download(*p), list_var1)
    # threads_pool.submit(lambda p: download(*p), list_var2)

    threads = [threads_pool.submit(lambda p: func(*p), list) for list in lists]
    # for t in threads:
    #     print(t.done())

    for future in as_completed(threads):
        # 使用as_completed方法一次取出所有任务的结果
        data = future.result()
        print(f"{data}")
    # wait方法可以让主线程阻塞，直到满足设定的要求
    wait(threads, return_when=ALL_COMPLETED)
```

## 多线程简单介绍

### 如果要运行的函数只有一个参数


```python
import time

from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed, wait, ALL_COMPLETED

def test(num):
    list = []
    for i in range(0, num):
        list.append(i)
    return sum(i for i in range(0, num))

def multi_threads_pool(num, func):
    """线程池下载"""
    threads_pool = ThreadPoolExecutor(num)
    threads = [threads_pool.submit(func, 10000)]
    for future in as_completed(threads):
        # 使用as_completed方法一次取出所有任务的结果
        data = future.result()
        print(f"{data}")
    # wait方法可以让主线程阻塞，直到满足设定的要求
    wait(threads, return_when=ALL_COMPLETED)
    
start = time.time()    
multi_threads_pool(4,test)
end = time.time()
print(f'耗时{end-start}秒')
```

    49995000
    耗时0.02398538589477539秒


### 如果运行的函数有多个参数


```python
import time

from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed, wait, ALL_COMPLETED

def test(num1, num2):
    list = []
    for i in range(0, num1):
        for j in range(0, num2):
            list.append(i + j)
    sum1 = sum(y for y in range(0, num1))
    sum2 = sum(n for n in range(0, num2))
    return (sum1+sum2)

def multi_threads_pool(num, func):
    """线程池下载"""
    threads_pool = ThreadPoolExecutor(num)
    threads = [threads_pool.submit(func, 1000, 1000) ]
    for future in as_completed(threads):
        # 使用as_completed方法一次取出所有任务的结果
        data = future.result()
        print(f"{data}")
    # wait方法可以让主线程阻塞，直到满足设定的要求
    wait(threads, return_when=ALL_COMPLETED)
    
start = time.time()    
multi_threads_pool(4,test)
end = time.time()
print(f'耗时{end-start}秒')
```

### 如果执行多组求和

- 方法1：按需传参


```python
import time

from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed, wait, ALL_COMPLETED

def test(num1, num2):
    list = []
    for i in range(0, num1):
        for j in range(0, num2):
            list.append(i + j)
    sum1 = sum(y for y in range(0, num1))
    sum2 = sum(n for n in range(0, num2))
    return (sum1+sum2)

def multi_threads_pool(num, func, list1, list2):
    """线程池下载"""
    threads_pool = ThreadPoolExecutor(num)
    threads = [threads_pool.submit(func, num1, num2) for num1, num2 in zip(list1, list2)]
    for future in as_completed(threads):
        # 使用as_completed方法一次取出所有任务的结果
        data = future.result()
        print(f"{data}")
    # wait方法可以让主线程阻塞，直到满足设定的要求
    wait(threads, return_when=ALL_COMPLETED)
    
start = time.time()
list1 = [i for i in range(10, 100, 10)]
list2 = [i for i in range(20, 200, 20)]
print(list1,list2)

multi_threads_pool(4,test, list1, list2)
end = time.time()
print(f'耗时{end-start}秒')
```

    [10, 20, 30, 40, 50, 60, 70, 80, 90] [20, 40, 60, 80, 100, 120, 140, 160, 180]
    2205
    3940
    970
    235
    6175
    8910
    12145
    15880
    20115
    耗时0.04497194290161133秒



```python
list1 = [i for i in range(10, 100, 10)]
list2 = [i for i in range(20, 200, 20)]
print(list1,list2)

mylist = []
for a, b in zip(list1, list2):
    mylist.append([a, b])
print(mylist)
```

    [10, 20, 30, 40, 50, 60, 70, 80, 90] [20, 40, 60, 80, 100, 120, 140, 160, 180]
    [[10, 20], [20, 40], [30, 60], [40, 80], [50, 100], [60, 120], [70, 140], [80, 160], [90, 180]]


- 方法2：将参数放入`lsit`，借助`lambda`函数


```python
import time

from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed, wait, ALL_COMPLETED

def test(num1, num2):
    list = []
    for i in range(0, num1):
        for j in range(0, num2):
            list.append(i + j)
    sum1 = sum(y for y in range(0, num1))
    sum2 = sum(n for n in range(0, num2))
    return (sum1+sum2)

def multi_threads_pool(num, func, lists):
    """线程池下载"""
    threads_pool = ThreadPoolExecutor(num)
    threads = [threads_pool.submit(lambda p: func(*p), list) for list in lists]
    for future in as_completed(threads):
        # 使用as_completed方法一次取出所有任务的结果
        data = future.result()
        print(f"{data}")
    # wait方法可以让主线程阻塞，直到满足设定的要求
    wait(threads, return_when=ALL_COMPLETED)

    
list1 = [i for i in range(10, 100, 10)]
list2 = [i for i in range(20, 200, 20)]

mylist = []
for a, b in zip(list1, list2):
    mylist.append([a, b])
print(mylist)

start = time.time()    
multi_threads_pool(4,test, mylist)
end = time.time()
print(f'耗时{end-start}秒')
```

    [[10, 20], [20, 40], [30, 60], [40, 80], [50, 100], [60, 120], [70, 140], [80, 160], [90, 180]]
    970
    2205
    235
    3940
    6175
    12145
    8910
    20115
    15880
    耗时0.046973466873168945秒


## 多进程


```python
import time,os

from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed, wait, ALL_COMPLETED

def test(num1, num2):
    list = []
    for i in range(0, num1):
        for j in range(0, num2):
            list.append(i + j)
    sum1 = sum(y for y in range(0, num1))
    sum2 = sum(n for n in range(0, num2))
    print('启动下载进程，进程号[%d].' % os.getpid())
    return ((sum1+sum2), '进程号'+str(os.getpid()))

def multi_process_pool(num, func, list1, list2):
    '''进程池'''
    processes_pool = ProcessPoolExecutor(max_workers=num)
#     processes = [processes_pool.submit(lambda p: func(*p), list) for list in lists]
    processes = [processes_pool.submit(func, num1, num2) for num1, num2 in zip(list1, list2)]
    for process in as_completed(processes):
        # 使用as_completed方法一次取出所有任务的结果
        data = process.result()
        print(f"{data}")

    # wait方法可以让主线程阻塞，直到满足设定的要求
    wait(processes, return_when=ALL_COMPLETED)

    
list1 = [i for i in range(10, 100, 10)]
list2 = [i for i in range(20, 200, 20)]

mylist = []
for a, b in zip(list1, list2):
    mylist.append([a, b])
print(mylist)

start = time.time()    
multi_process_pool(4,test, list1, list2)
end = time.time()
print(f'耗时{end-start}秒')
```

```
[[10, 20], [20, 40], [30, 60], [40, 80], [50, 100], [60, 120], [70, 140], [80, 160], [90, 180]]
启动下载进程，进程号[4036].
启动下载进程，进程号[4037].
启动下载进程，进程号[4035].
启动下载进程，进程号[4038].
启动下载进程，进程号[4036].
启动下载进程，进程号[4035].
启动下载进程，进程号[4037].
启动下载进程，进程号[4036].
启动下载进程，进程号[4035].
(2205, '进程号4037')
(970, '进程号4036')
(235, '进程号4035')
(3940, '进程号4038')
(6175, '进程号4036')
(12145, '进程号4035')
(8910, '进程号4037')
(15880, '进程号4036')
(20115, '进程号4035')
耗时0.12399792671203613秒
```

## Bugs

- bug1
    ```
    concurrent.futures.process.BrokenProcessPool: A process in the process pool was terminated abruptly while the future was running or pending
    进程池中的进程在未来运行或挂起时突然终止
    ```

    **原因**：
    - 使用进程池会出现此错误，单进程不会

    **解决方法：**
    把代码加到main中执行

    ```
    def main():
        your code
        pass
    if __name__ = '__main__':
        main()
    ```
    - 环境问题： Windows Jupyter Notebook环境导致，Ubuntu不会出现

- bug2

    ```
    _pickle.PicklingError: Can't pickle <function parse_poster at 0x0B2BFA98>: it's not the same object as __main__.parse_poster
    ```
    **解决方法：**
    本人使用Pychram作为编译环境，重新run`poster_parse.py`文件，顺利执行  
    在 Window Jupyter Notebook 环境会出现bug1 

## 多进程&多线程爬取

完整代码如下：
```python
# poster_parse.py
import json
import os
import time
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed, wait, ALL_COMPLETED

import requests


def download(src, img_name, query):
    """" 下载图片 """
    # print('启动下载进程，进程号[%d].' % os.getpid())
    dir = './' + query + '/' + str(img_name) + '.jpg'
    try:
        pic = requests.get(src, timeout=10)
        fp = open(dir, 'wb')
        fp.write(pic.content)
        fp.close()
        return '开始下载:' + str(img_name), src
    except requests.exceptions.ConnectionError:
        return 'ConnectionError:' + str(img_name) + '无法下载!'
    except OSError:
        return 'OSError:' + str(img_name) + '无法下载!'


def multi_threads_pool(num, func, lists):
    """线程池下载"""
    threads_pool = ThreadPoolExecutor(num)
    threads = [threads_pool.submit(lambda p: func(*p), list) for list in lists]

    for future in as_completed(threads):
        # 使用as_completed方法一次取出所有任务的结果
        data = future.result()
        print(f"{data}")

    # wait方法可以让主线程阻塞，直到满足设定的要求
    wait(threads, return_when=ALL_COMPLETED)


def multi_process_pool(num, func, lists, query):
    '''进程池'''
    processes_pool = ProcessPoolExecutor(max_workers=num)
    # processes = [processes_pool.submit(lambda p: func(*p), list) for list in lists]
    processes = [processes_pool.submit(func, list, query) for list, query in zip(lists, [query] * len(lists))]
    for process in as_completed(processes):
        # 使用as_completed方法一次取出所有任务的结果
        data = process.result()
        print(f"{data}")

    # wait方法可以让主线程阻塞，直到满足设定的要求
    wait(processes, return_when=ALL_COMPLETED)


def crawling_poster(url, query):
    '''解析url, 获取元素'''
    html = requests.get(url).text
    response = json.loads(html, encoding='utf-8')
    srcs = []
    ids = []
    for image in response['images']:
        srcs.append(image['src'])
        ids.append(image['id'])
    lists = []
    for src, id, q in zip(srcs, ids, [query] * len(srcs)):
        lists.append([src, id, q])
    print('启动下载进程，进程号[%d].' % os.getpid())
    start = time.time()
    #线程池下载图片
    multi_threads_pool(8, download, lists)

    #单线程下载图片
    for src, id in zip(srcs, ids):
        download(src, id, query)
    end = time.time()
    print(f'本页图片下载耗时{end - start}秒')
    return url

def main():
    query = '王祖贤'

    # 创建文件夹
    folder_name = query
    if not os.path.exists(folder_name):
        os.makedirs(folder_name)

    urls = []
    for i in range(0, 500, 20):
        urls.append('https://www.douban.com/j/search_photo?q=' + query + '&limit20&start=' + str(i))


    start = time.time()

    # 进程池
#     multi_process_pool(2, crawling_poster, urls, query)

    # 单进程
    for url in urls:
        print(crawling_poster(url, query))

    end = time.time()
    print(f'\nDownload Finished!\n耗时{end - start}秒')

if __name__ == '__main__':
    main()
```
结果如下：
结果内容太多，删除冗余，保留一点过程
```
    https://www.douban.com/j/search_photo?q=王祖贤&limit20&start=0
    启动下载进程，进程号[15668].
     启动下载进程，进程号[15668].
    ('开始下载:642504623', 'https://img3.doubanio.com/view/photo/thumb/public/p642504623.jpg')
    ...
    ('开始下载:642504596', 'https://img3.doubanio.com/view/photo/thumb/public/p642504596.jpg')
    本页图片下载耗时0.0秒
    https://www.douban.com/j/search_photo?q=王祖贤&limit20&start=20
    启动下载进程，进程号[15668].
    本页图片下载耗时0.0秒
    ...
    https://www.douban.com/j/search_photo?q=王祖贤&limit20&start=480
    
    Download Finished!
    耗时71.71922850608826秒
```

![title](https://raw.githubusercontent.com/hufe09/GitNote-Images/master/gitnote/2019/05/14/1557765183198-1557765183221.png)